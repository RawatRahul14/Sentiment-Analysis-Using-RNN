{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60aa5522",
   "metadata": {},
   "source": [
    "## Task - 1: Understanding Sentiment Aalysis and RNNS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f27da",
   "metadata": {},
   "source": [
    "**Q1. What is sentiment analysis and it's applications?**\n",
    "\n",
    "Sentiment analysis is the process of enabling a computer to determine the emotional tone—negative, neutral, or positive of a user's message based on digital text.\n",
    "\n",
    "Applications of sentiment analysis:\n",
    "- **Social Media Monitering:**\n",
    "    - Tracks public opinion on brands, products or events.\n",
    "    - Identify viral trends or customer sentiment shifts in real life.\n",
    "\n",
    "- **Customer Feedeback Analysis:**\n",
    "    - Analyze reviews to help understand satisfaction levels.\n",
    "    - Prioritize complaints or negative experiences for faster resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c83445",
   "metadata": {},
   "source": [
    "**Q2. How RNNs differ from traditional feedforward neural networks?**\n",
    "\n",
    "A Recurrent Neural Network (RNN) is specially designed to handle sequential data or time series data, where the output at a particular time depends on previous inputs.\n",
    "\n",
    "They remember previous inputs using internal memory, which helps in learning patterns over time. For example, traditional neural networks can recognize digits in images, while RNNs are used for tasks like predicting the next word in a sentence, speech recognition, or stock price forecasting, where the order and context of data matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a433f",
   "metadata": {},
   "source": [
    "**Q3. The concept of hidden states and how information is passed through time steps in RNNs.**\n",
    "\n",
    "***Concept of Hidden States in RNNs***\n",
    "\n",
    "In RNNs, a hidden state is a memory-like vector that stores information from previous time steps. It's the key component that allows RNNs to handle sequential data like sentences, time series, or speech.\n",
    "\n",
    "***How Information is Passed Through Time Steps***\n",
    "1. At **Time Step t = 1**:\n",
    "    - Inputs: $x_1$\n",
    "    - Hidden state: $h_0$ (usually initialized as zeros)\n",
    "    - RNN Computes\n",
    "\n",
    "    $$\n",
    "    h_1 = \\tanh(W_{xh} \\cdot x_1 + W_{hh} \\cdot h_0 + b)\n",
    "    $$\n",
    "\n",
    "    - $h_1$ now stores the information from $x_1$\n",
    "\n",
    "2. At **Time Step t = 2**:\n",
    "    - Input: $x_2$\n",
    "    - Previous hidden state: $h_1$\n",
    "    - RNN computes:\n",
    "\n",
    "    $$\n",
    "    h_2 = \\tanh(W_{xh} \\cdot x_2 + W_{hh} \\cdot h_1 + b)\n",
    "    $$\n",
    "\n",
    "    - Now $h_2$ contains both $x_2$ and prior context $h_1$\n",
    "\n",
    "3. This continous for all time steps:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b)\n",
    "$$\n",
    "\n",
    "Each hidden state carries the essence of all previous inputs, making the network capable of remembering sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a877da",
   "metadata": {},
   "source": [
    "**Q4. Common issues with RNNs such as vanishing and exploding gradients.**\n",
    "\n",
    "RNNs often face two major issues:\n",
    "\n",
    "- ***Vanishing gradients***: Gradients become too small during training, making it hard to learn long-term dependencies.\n",
    "- ***Exploding gradients***: Gradients become too large, causing unstable training or model crash.\n",
    "\n",
    "These problems occur during backpropagation through many time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27145c7",
   "metadata": {},
   "source": [
    "## Task - 2: Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f670dc",
   "metadata": {},
   "source": [
    "#### Loading the IMDB dataset from the TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f681d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing suitable packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c47cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset, the tokenization is already done by the TensorFlow\n",
    "vocab_size = 10_000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c2e704d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the X_train dataset: (25000,) and Y_train: (25000,)\n",
      "Shape of the X_test dataset: (25000,) and Y_test: (25000,)\n"
     ]
    }
   ],
   "source": [
    "# Shape of the training and test datasets\n",
    "print(f\"Shape of the X_train dataset: {x_train.shape} and Y_train: {y_train.shape}\")\n",
    "print(f\"Shape of the X_test dataset: {x_test.shape} and Y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d36a8920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]: 1\n",
      "[1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]: 0\n",
      "[1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]: 0\n",
      "[1, 4, 2, 2, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 2, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 2, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 2, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 2, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 2, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 2, 192, 5, 1219, 3890, 19, 2, 217, 4122, 1710, 537, 2, 1236, 5, 736, 10, 10, 61, 403, 9, 2, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 2, 594, 7, 5168, 94, 9096, 3987, 2, 11, 2, 4, 538, 7, 1795, 246, 2, 9, 2, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 2, 19, 49, 7, 4, 1885, 2, 1118, 25, 80, 126, 842, 10, 10, 2, 2, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 2, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 2, 21, 29, 9, 2841, 23, 4, 1010, 2, 793, 6, 2, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 2, 8, 124, 4, 882, 4, 882, 496, 27, 2, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 2, 1311, 8, 4, 2, 7, 31, 7, 2, 91, 2, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 2, 1775, 3353, 2, 1846, 4, 2, 7, 154, 5, 4, 518, 53, 2, 2, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 2, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 2, 4, 2, 2, 9, 242, 4, 91, 1202, 2, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 2, 13, 188, 1076, 3222, 19, 4, 2, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 2, 13, 2, 14, 280, 13, 219, 4, 2, 431, 758, 859, 4, 953, 1052, 2, 7, 5991, 5, 94, 40, 25, 238, 60, 2, 4, 2, 804, 2, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 2, 23, 12, 1685, 195, 25, 238, 60, 796, 2, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574]: 1\n",
      "[1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 2, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113]: 0\n"
     ]
    }
   ],
   "source": [
    "# First few examples from the train dataset\n",
    "for i in range(5):\n",
    "    print(f\"{x_train[i]}: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b996b732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length in training data: 2494\n"
     ]
    }
   ],
   "source": [
    "# Finding the maximum review length\n",
    "maxlen_review = max(len(review) for review in x_train)\n",
    "print(f\"Maximum review length in training data: {maxlen_review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76ac66",
   "metadata": {},
   "source": [
    "> **Note**: Using the *maximum length* can lead to large input sizes and **slower training**, especially if a few reviews are extremely long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4100550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using maxlen = 467 for padding.\n"
     ]
    }
   ],
   "source": [
    "# Setting maxlen to the 90th percentile of review lengths to capture most data while avoiding very long outliers\n",
    "review_lengths = [len(review) for review in x_train]\n",
    "maxlen = int(np.percentile(review_lengths, 90))\n",
    "\n",
    "print(f\"Using maxlen = {maxlen} for padding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9245831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the padding on the training and testing data\n",
    "x_train_padded = pad_sequences(x_train, maxlen = maxlen, padding = \"post\", truncating = \"post\")\n",
    "x_test_padded = pad_sequences(x_test, maxlen = maxlen, padding = \"post\", truncating = \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea54e267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded x_train: (25000, 467)\n",
      "Shape of padded x_test: (25000, 467)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of padded x_train: {x_train_padded.shape}\")\n",
    "print(f\"Shape of padded x_test: {x_test_padded.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
